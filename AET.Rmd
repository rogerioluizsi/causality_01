---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 
```{r}
install.packages(c('ggplot2','glmnet', 'grf', 'sandwich', 'devtools', 'dplyr', 'DescTools', 'caret', 'MatchIt'))
library(devtools)
install_github("swager/balanceHD")
```


```{r}

# Set seed for reproducibility
set.seed(202010)
library(ggplot2)   # plot
library(glmnet)    # lasso
library(grf)       # generalized random forests
library(sandwich)  # for robust CIs
library(devtools)  # install from GitHub 
library(balanceHD) # approximate residual balancing
library(dplyr)
library(ROCit)
library(caret)
library(DescTools)
library(MatchIt)
```


```{r}
#load data
students<- read.csv('~/features_engeneering/ALL_STUDENTS_RAW.csv')
#school<- read.csv('~/features_engeneering/ALL_SCHOOLS.csv')

```

Choose the dataset, year and type of school
```{r}
# Filtering
df<- students%>% filter(IN_TP_ESCOLA == 'Municipal+Estadual', CO_ANO==2018)
```


```{r}

df$RACA_NAO_DECLARADA<- if_else(df$TP_COR_RACA ==0, 1, 0)
df$RACA_BRANCA<- if_else(df$TP_COR_RACA ==1, 1, 0)
df$RACA_PRETA<- if_else(df$TP_COR_RACA ==2, 1, 0)
df$RACA_PARDA<- if_else(df$TP_COR_RACA ==3, 1, 0)
df$RACA_AMARELA<- if_else(df$TP_COR_RACA ==4, 1, 0)

#INDIANS ARE REPRESENTED WHEN ZERO IN ALL OTHER ONES

```

Pre processing fuctions

```{r}
#Functions
build_target <- function(dataset){
  dataset<- dataset%>%mutate(
    Y_bin = ntile(Y, 4))
  dataset$Y_bin<- if_else(dataset$Y_bin == 4, 1, 0)
  return(dataset%>%select(-Y))
}

clip_tail<- function(dataset) {
  for (i in colnames(dataset)) {
    #print(i)
    if (is.numeric(dataset[[i]]) & (length(dataset[[i]]%>%unique()> 2))){
      #print(i)
      dataset[[i]]<- Winsorize( dataset[[i]],  probs = c(0.025, 0.975), na.rm = FALSE, type = 7)
    }
  }
  return(dataset)
}  

drop_col_hight_mode<- function(dataset){
  for (i in colnames(dataset)){
    prop_mode<- sort(-table(dataset[[i]]))[1]/nrow(dataset)*(-1)# get mode value
    if (prop_mode > 0.9){
      dataset<- dataset%>%dplyr::select(-i)
      print(i)
    }
  }
  return(dataset)
}


```

Build the final dataset

```{r}

#Set the threshold for treatment variable - Treatment is father who have graduate level or above
df$EDU_PAI<- if_else(df$EDU_PAI >3, 1, 0)



#selecting features set
num_features_names<- c('RENDA_PERCAPITA', 'NU_IDADE', 'TITULACAO')
bin_features_names<- c('IN_INFRA_ELEMENTAR', 'IN_INFRA_BASICA', 'IN_INFRA_ADEQUADA', 'IN_INFRA_AVANCADA',
                       'RACA_NAO_DECLARADA','RACA_BRANCA','RACA_PRETA','RACA_PARDA','RACA_AMARELA', 'TP_SEXO')

# Extracting indicator variables
bin_features <- df[,bin_features_names]

# Extracting outcome and treatment
outcome <- df$NU_NOTA_GERAL
treatment <- df$EDU_PAI

#Trimming long tailed covariates
clipped<- clip_tail(df[num_features_names])

# Extracting and scaling numeric features
#scaled_num_features <- scale(clipped[,])
pp = preProcess(clipped, method = "range")
scaled<- predict(pp, clipped)

scaled_num_features <- data.matrix(scaled)

#New dataset
df_mod <- data.frame(scaled_num_features, bin_features, W=treatment, Y=outcome)

#Build outcome (Target) covariate and take off covariates with hight mode occourrence
df_mod<-build_target(df_mod)
#drop_col_hight_mode(df_mod)


```

Naive treatment (father education) effects
```{r}
difference_in_means <- function(dataset) {
  treated_idx <- which(dataset$W == 1)
  control_idx <- which(dataset$W == 0)
  
  # Filter treatment / control observations, pulls outcome variable as a vector
  y1 <- dataset[treated_idx, "Y_bin"] # Outcome in treatment grp
  y0 <- dataset[control_idx, "Y_bin"] # Outcome in control group
  
  #print(treated_idx)
  n1 <- sum(dataset[,"W"])     # Number of obs in treatment
  n0 <- sum(1 - dataset[,"W"]) # Number of obs in control
  
  # Difference in means is ATE
  tauhat <- mean(y1) - mean(y0)
  
  # 95% Confidence intervals
  se_hat <- sqrt( var(y0)/(n0-1) + var(y1)/(n1-1) )
  lower_ci <- tauhat - 1.96 * se_hat
  upper_ci <- tauhat + 1.96 * se_hat
  
  return(c(ATE = tauhat, lower_ci = lower_ci, upper_ci = upper_ci))
}

tauhat_rct <- difference_in_means(df_mod)
print(tauhat_rct)
```

Logistic Regression Model with interactions at whole dataset

```{r}

p_logistic.fit <- glm(Y_bin ~ .+RENDA_PERCAPITA*W,
                      family = binomial(logit), data= df_mod)
p_logistic <- predict(p_logistic.fit, type = "response")

p <- data.frame(df_mod$Y_bin, p_logistic)

ggplot(p
       ,aes(p_logistic,  fill = as.factor(df_mod.Y_bin))) +
  geom_density(alpha = 0.7) 
  

## Warning: package 'ROCit' was built under R version 3.5.2

ROCit_obj <- rocit(score=p_logistic,class=df_mod$Y_bin)
plot(ROCit_obj)

ksplot(ROCit_obj)

summary(ROCit_obj)
tauhat_lr = as.numeric(coef(p_logistic.fit)["W"])
print(tauhat_lr)
summary(p_logistic.fit)
```


Computing Propensity Score 

```{r}
Xmod = df_mod[,!names(df_mod) %in% c("Y_bin", "W")]
Ymod = df_mod$Y_bin
Wmod = df_mod$W

# Computing the propensity score by logistic regression of W on X.
p_logistic.fit <- glm(Wmod ~ as.matrix(Xmod), family = binomial(logit))
p_logistic <- predict(p_logistic.fit, type = "response")

hist(p_logistic, main = "Histogram: Logistic Regression Propensity Scores"
     , xlab = "Propensity Score", col = "cornflowerblue", las = 1)

```
```{r}
plot(smooth.spline(x = p_logistic, y = Wmod, df = 4)
     , xlab = "Propensity Score (Logistic Regression)", ylab = "Prob. Treated (W)"
     , col = adjustcolor("black", alpha.f=0.4), pch=19, las = 1)
abline(0, 1, lty="dashed")
```
```{r}
dt <- data.frame(Wmod, p_logistic)

ggplot(dt
       ,aes(p_logistic,  fill = as.factor(Wmod))) +
  geom_density(alpha = 0.7) +
  xlim(0, 1)+
  geom_vline(aes(xintercept=mean(dt%>%filter(Wmod==0)%>%pull(p_logistic))),
            color="pink", linetype="dashed", size=1)+
  geom_vline(aes(xintercept=mean(dt%>%filter(Wmod==1)%>%pull(p_logistic))),
            color="blue", linetype="dashed", size=1)+
  scale_fill_discrete("Father Education")
```


Inverse-propensity score weighting - Here the propensity score serves a single-dimensional variable that summarizes how observable affect the treatment probability.

```{r}
ipw <- function(dataset, p) {
  W <- dataset$W
  Y <- dataset$Y_bin
  G <- ((W - p) * Y) / (p * (1 - p))
  tau.hat <- mean(G)
  se.hat <- sqrt(var(G) / (length(G) - 1))
  c(ATE=tau.hat, lower_ci = tau.hat - 1.96 * se.hat, upper_ci = tau.hat + 1.96 * se.hat)
}

tauhat_logistic_ipw <- ipw(df_mod, p_logistic)
print(tauhat_logistic_ipw)

```

Weighted LR on W

```{r}
prop_score_lr <- function(dataset, p) {
  # Pulling relevant columns
  W <- dataset$W
  Y <- dataset$Y_bin
  # Computing weights
  weights <- (W / p) + ((1 - W) / (1 - p))
  
  # LR
  lr.fit <- glm(Y ~ W,family = binomial(logit), data = dataset, weights = weights)
  tau.hat = c(ATE =as.numeric(coef(lr.fit)["W"]))
  
}

tauhat_pscore_lr<- prop_score_lr(df_mod, p_logistic)
print(tauhat_pscore_lr)
```


Doubly Robust methods

combines both parts (regression and weighting) in an attempt to ameliorate the sensitivity to misspecification of models

1 - First part modeled the conditional mean of outcomes given covariates and treatment
2 - Second part rebalanced  the sample using propensity score 


```{r}
aipw_lr <- function(dataset, p) {
  
  lr.fit = glm(Y_bin ~ W * ., data = dataset, family = binomial(logit))
  
  dataset.treatall = dataset
  dataset.treatall$W = 1
  treated_pred = predict(lr.fit, dataset.treatall)
  
  dataset.treatnone = dataset
  dataset.treatnone$W = 0
  control_pred = predict(lr.fit, dataset.treatnone)
  
  actual_pred = predict(lr.fit, dataset)
  
  G <- treated_pred - control_pred +
    ((dataset$W - p) * (dataset$Y_bin - actual_pred)) / (p * (1 - p))
  tau.hat <- c(ATE=mean(G))
  
}

tauhat_lin_logistic_aipw <- aipw_lr(df_mod, p_logistic)
print(tauhat_lin_logistic_aipw)
```

Random Forest
```{r}

Xmod = df_mod[,!names(df_mod) %in% c("Y_bin", "W")]
Ymod = df_mod$Y_bin
Wmod = df_mod$W

cf = causal_forest(Xmod, Ymod, Wmod, num.trees = 100)

```


Let's check if Causal Forest model build best probabilities
```{r}
p_rf = cf$W.hat

hist(p_rf)
```

```{r}
plot(smooth.spline(p_rf, Wmod, df = 4))
abline(0, 1)
```
```{r}

dt <- data.frame(Wmod, p_rf)
ggplot(dt
       ,aes(p_rf,  fill = as.factor(Wmod))) +
  geom_density(alpha = 0.7) +
  xlim(0, 1)+
  geom_vline(aes(xintercept=mean(dt%>%filter(Wmod==0)%>%pull(p_logistic))),
            color="pink", linetype="dashed", size=1)+
  geom_vline(aes(xintercept=mean(dt%>%filter(Wmod==1)%>%pull(p_logistic))),
            color="blue", linetype="dashed", size=1)+
  scale_fill_discrete("Father Education")
```


```{r}
plot(p_rf, p_logistic)
abline(0, 1)
```
```{r}
# compare the log likelihoods (bigger is better)
loglik = c(LR=mean(Wmod * log(p_logistic) + (1 - Wmod) * log(1 - p_logistic)),
           RF=mean(Wmod * log(p_rf) + (1 - Wmod) * log(1 - p_rf)))
loglik
```

```{r}
tauhat_ols_rf_aipw = ipw(df_mod, p_rf)

tauhat_ols_rf_aipw
```

```{r}
tauhat_pscore_rf<- prop_score_lr(df_mod, p_rf)
print(tauhat_pscore_rf)
```



Propensity score matching - Using Logistic Regression

Letâ€™s calculate the mean for each covariate by the treatment status:

```{r}
teste<- df_mod %>%
  group_by(W) %>%
  select(one_of(colnames(df_mod))) %>%
  summarise_all(funs(mean(., na.rm = T)))

df2 <- data.frame(t(teste[-1]))
colnames(df2) <- teste[, 1]

```

Carrying out t-testes to evaluate whether these means are staticaly significant
```{r}
lapply(num_features_names, function(v) {
    t.test(df_mod[, v] ~ df_mod[, 'W'])
})

lapply(bin_features_names, function(v) {
    t.test(df_mod[, v] ~ df_mod[, 'W'])
})
```
```{r}
mod_match <- matchit(W ~ RENDA_PERCAPITA+NU_IDADE+TITULACAO+IN_INFRA_ELEMENTAR+IN_INFRA_BASICA+IN_INFRA_ADEQUADA+IN_INFRA_AVANCADA+RACA_NAO_DECLARADA+RACA_AMARELA+RACA_PARDA+RACA_BRANCA+RACA_PRETA+TP_SEXO,
                     method = "nearest", data = df_mod)
```

